- [x] add working calibration
- [ ] add more calibration responses based on real assistant 
- [x] add working evaluation
- [x] add score system
    - [x] per question
    - [x] per category
- [x] add some of our scenarios https://github.tools.sap/kyma/ai-force/tree/main/examples
- [ ] add real live scenarios (especially scenarios our assitant struggles with)
- [ ] have an export to json
- [ ] run tests async

adjust to our next assistant
- [ ] align the scenarios better to what the assistant uses as tools

bigger problems
- [ ] can we create a real 'calibration' or can we just pick the best model for the eval?
- [ ] how do we version our validation by category if we keep changing our scenarios?

nice to have
- [ ] add unit tests
- [ ] make the differences for AB-testing more obvious
- [ ] visualize the results